# -*- coding: utf-8 -*-
"""p44140 sravani.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17L4MUPAP26BcuvC7_idwrOavxYVi44Uz
"""

# prompt: import pandas

import pandas as pd

# prompt: import /content/Jinka Jayalakshmi Venkata Sravani - revolutioncart_data.xlsx

df = pd.read_excel('/content/Jinka Jayalakshmi Venkata Sravani - revolutioncart_data.xlsx')
print(df.head())

# prompt: df

df

# prompt: consider monthly_revenue as y and others as x

y = df['monthly_revenue']
X = df.drop('monthly_revenue', axis=1)

# prompt: split x and y into train and test

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# prompt: make linear regression model for x_train and y_train

from sklearn.linear_model import LinearRegression

# Create a Linear Regression model
model = LinearRegression()

# Train the model on the training data
model.fit(X_train, y_train)

# prompt: show coefficient and intercept

# Print the coefficients and intercept
print("Coefficients:", model.coef_)
print("Intercept:", model.intercept_)

# prompt: model.score (x_test, y_test)

print(model.score(X_test, y_test))

# prompt: model.score (x_train, y_train)

print(model.score(X_train, y_train))

# prompt: do cross validation

from sklearn.model_selection import cross_val_score

# Perform cross-validation
cv_scores = cross_val_score(model, X, y, cv=5)  # You can change the number of folds (cv)

# Print the cross-validation scores
print("Cross-validation scores:", cv_scores)
print("Average cross-validation score:", cv_scores.mean())

# prompt: dump model

import pickle

# Save the model to a file
filename = 'linear_regression_model.sav'
pickle.dump(model, open(filename, 'wb'))

# prompt: do prediction by taking some data from the above data

# Assuming you want to predict the monthly revenue for the first 5 rows in the dataset
new_data = X.iloc[:5]  # Select the first 5 rows for prediction

# Make predictions using the trained model
predictions = model.predict(new_data)

# Print the predictions
print("Predictions:", predictions)

# prompt: make lasso regression for x_train and y_train

from sklearn.linear_model import Lasso

# Create a Lasso Regression model
lasso_model = Lasso(alpha=0.1)  # You can adjust the alpha parameter

# Train the model on the training data
lasso_model.fit(X_train, y_train)

# Print the coefficients and intercept
print("Lasso Coefficients:", lasso_model.coef_)
print("Lasso Intercept:", lasso_model.intercept_)

# Evaluate the model on the test data
print("Lasso R-squared on test data:", lasso_model.score(X_test, y_test))

# Perform cross-validation for Lasso
lasso_cv_scores = cross_val_score(lasso_model, X, y, cv=5)

# Print the cross-validation scores for Lasso
print("Lasso Cross-validation scores:", lasso_cv_scores)
print("Lasso Average cross-validation score:", lasso_cv_scores.mean())

# prompt: lasso_model.score(X_test, y_test)

lasso_model.score(X_test, y_test)

# prompt: do cross validation for the lasso model

# Perform cross-validation for Lasso
lasso_cv_scores = cross_val_score(lasso_model, X, y, cv=5)

# Print the cross-validation scores for Lasso
print("Lasso Cross-validation scores:", lasso_cv_scores)
print("Lasso Average cross-validation score:", lasso_cv_scores.mean())

# prompt: make ridge regression with X_train and y_train

from sklearn.linear_model import Ridge

# Create a Ridge Regression model
ridge_model = Ridge(alpha=1.0)  # You can adjust the alpha parameter

# Train the model on the training data
ridge_model.fit(X_train, y_train)

# Print the coefficients and intercept
print("Ridge Coefficients:", ridge_model.coef_)
print("Ridge Intercept:", ridge_model.intercept_)

# Evaluate the model on the test data
print("Ridge R-squared on test data:", ridge_model.score(X_test, y_test))

# Perform cross-validation for Ridge
ridge_cv_scores = cross_val_score(ridge_model, X, y, cv=5)

# Print the cross-validation scores for Ridge
print("Ridge Cross-validation scores:", ridge_cv_scores)
print("Ridge Average cross-validation score:", ridge_cv_scores.mean())

# prompt: make elasticnet on X_train and y_train

from sklearn.linear_model import ElasticNet

# Create an ElasticNet model
elasticnet_model = ElasticNet(alpha=0.1, l1_ratio=0.5)  # You can adjust alpha and l1_ratio

# Train the model on the training data
elasticnet_model.fit(X_train, y_train)

# Print the coefficients and intercept
print("ElasticNet Coefficients:", elasticnet_model.coef_)
print("ElasticNet Intercept:", elasticnet_model.intercept_)

# Evaluate the model on the test data
print("ElasticNet R-squared on test data:", elasticnet_model.score(X_test, y_test))

# Perform cross-validation for ElasticNet
elasticnet_cv_scores = cross_val_score(elasticnet_model, X, y, cv=5)

# Print the cross-validation scores for ElasticNet
print("ElasticNet Cross-validation scores:", elasticnet_cv_scores)
print("ElasticNet Average cross-validation score:", elasticnet_cv_scores.mean())

# prompt: do cross validation on elasticnet model

from sklearn.model_selection import cross_val_score

# Perform cross-validation for ElasticNet
elasticnet_cv_scores = cross_val_score(elasticnet_model, X, y, cv=5)

# Print the cross-validation scores for ElasticNet
print("ElasticNet Cross-validation scores:", elasticnet_cv_scores)
print("ElasticNet Average cross-validation score:", elasticnet_cv_scores.mean())

# prompt: make random forest regressor model for the X_train and y_train

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Create a Random Forest Regression model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

# Train the model on the training data
rf_model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = rf_model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Random Forest Regression:")
print("Mean Squared Error:", mse)
print("R-squared:", r2)

# Perform cross-validation for Random Forest
rf_cv_scores = cross_val_score(rf_model, X, y, cv=5)

# Print the cross-validation scores for Random Forest
print("Random Forest Cross-validation scores:", rf_cv_scores)
print("Random Forest Average cross-validation score:", rf_cv_scores.mean())

# prompt: please do random forest hyperparamter optimization with methhurestic

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

# Define the parameter grid for hyperparameter optimization
param_grid = {
    'n_estimators': randint(50, 200),  # Number of trees in the forest
    'max_depth': randint(10, 50),  # Maximum depth of each tree
    'min_samples_split': randint(2, 10),  # Minimum number of samples required to split an internal node
    'min_samples_leaf': randint(1, 5),  # Minimum number of samples required to be at a leaf node
    'max_features': ['sqrt', 'log2'],  # Number of features to consider at every split
}


# Create a Random Forest Regression model
rf_model = RandomForestRegressor(random_state=42)


# Create a RandomizedSearchCV object
random_search = RandomizedSearchCV(
    estimator=rf_model,
    param_distributions=param_grid,
    n_iter=100,  # Number of parameter settings that are sampled
    scoring='neg_mean_squared_error',  # Use negative MSE for optimization
    cv=5,  # Number of cross-validation folds
    verbose=2,  # Increase verbosity for more information during the search
    random_state=42,
    n_jobs=-1  # Use all available CPU cores
)


# Fit the random search to the training data
random_search.fit(X_train, y_train)


# Print the best hyperparameters found
print("Best hyperparameters:", random_search.best_params_)

# Evaluate the best model on the test data
best_rf_model = random_search.best_estimator_
y_pred = best_rf_model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Best Random Forest Regression:")
print("Mean Squared Error:", mse)
print("R-squared:", r2)

# prompt: by passing the best parameter make random forest classifier

# Assuming you have already performed the RandomizedSearchCV and obtained the best parameters:
best_params = random_search.best_params_

# Create a Random Forest Regression model with the best hyperparameters
best_rf_model = RandomForestRegressor(**best_params, random_state=42)

# Train the model on the training data
best_rf_model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = best_rf_model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Best Random Forest Regression (with best parameters):")
print("Mean Squared Error:", mse)
print("R-squared:", r2)

# prompt: do cross validation for the best_rf_model

# Perform cross-validation for the best Random Forest model
best_rf_cv_scores = cross_val_score(best_rf_model, X, y, cv=5)

# Print the cross-validation scores for the best Random Forest model
print("Best Random Forest Cross-validation scores:", best_rf_cv_scores)
print("Best Random Forest Average cross-validation score:", best_rf_cv_scores.mean())

# prompt: !pip install deap

!pip install deap

# prompt: with Best hyperparameters found in the above data find best accuracy model

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Assuming you have already performed the RandomizedSearchCV and obtained the best parameters:
best_params = random_search.best_params_

# Create a Random Forest Regression model with the best hyperparameters
best_rf_model = RandomForestRegressor(**best_params, random_state=42)

# Train the model on the training data
best_rf_model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = best_rf_model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Best Random Forest Regression (with best parameters):")
print("Mean Squared Error:", mse)
print("R-squared:", r2)

# You can also use other metrics like MAE or RMSE depending on your needs.
# The R-squared value indicates the proportion of variance in the dependent variable
# that is predictable from the independent variables. A higher R-squared generally
# indicates a better fit of the model to the data.